{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b7d466",
   "metadata": {
    "id": "43b7d466"
   },
   "source": [
    "# <center> CS 178: Machine Learning &amp; Data Mining </center>\n",
    "## <center> Homework 4: Due Friday 26 May 2023 (11:59pm) </center>\n",
    "### <center> Version 1.1 (Last Modified: 16 May 2023) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0921f03",
   "metadata": {
    "id": "a0921f03"
   },
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "\n",
    "This homework (and many subsequent ones) will involve data analysis and reporting on methods and results\n",
    "using Python code. You will submit a **single PDF file** that contains everything to Gradescope. This includes any text you wish to include to describe your results, the complete code snippets of how you attempted each problem, any figures that were generated, and scans of any work on paper that you wish to include. It is important that you include enough detail that we know how you solved the problem, since otherwise we will be unable to grade it.\n",
    "\n",
    "\n",
    "Your homeworks will be given to you as Jupyter notebooks containing the problem descriptions and some template code that will help you get started. You are encouraged to use these starter Jupyter notebooks to complete your assignment and to write your report. This will help you not only ensure that all of the code for the solutions is included, but also will provide an easy way to export your results to a PDF file (for example, doing *print preview* and *printing to pdf*). I recommend liberal use of Markdown cells to create headers for each problem and sub-problem, explaining your implementation/answers, and including any mathematical equations. For parts of the homework you do on paper, scan it in such that it is legible (there are a number of free Android/iOS scanning apps, if you do not have access to a scanner), and include it as an image in the Jupyter notebook.\n",
    "\n",
    "If you have any questions/concerns about using Jupyter notebooks, ask us on EdD. If you decide not to use Jupyter notebooks, but go with Microsoft Word or Latex to create your PDF file, make sure that all of the answers can be generated from the code snippets included in the document.\n",
    "\n",
    "### Summary of Assignment: 100 total points\n",
    "- Problem 1: Drawing a Decision Tree (15 points)\n",
    "- Problem 2: Computing the Gini Index (15 points)\n",
    "- Problem 3: Implementing Decision Trees (50 points)\n",
    "    - Problem 3.1: `class_prob_vector` (5 points)\n",
    "    - Problem 3.2: `leaf_condition` (10 points)\n",
    "    - Problem 3.3: `gini_score` (10 points)\n",
    "    - Problem 3.4: `find_best_split` (15 points)\n",
    "    - Problem 3.5: `build_tree` (10 points)\n",
    "- Problem 4: Experimenting with Sklearn (15 points)\n",
    "    - Problem 4.1: Training a small DT (5 points)\n",
    "    - Problem 4.2: Varying depth (5 points)\n",
    "    - Problem 4.3: Varying min_leaf (5 points)\n",
    "- Statement of Collaboration (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cdb14",
   "metadata": {
    "id": "648cdb14"
   },
   "source": [
    "Before we get started, let's import some libraries that you will make use of in this assignment. Make sure that you run the code cell below in order to import these libraries.\n",
    "\n",
    "**Important: In the code block below, we set `seed=1234`. This is to ensure your code has reproducible results and is important for grading. Do not change this. If you are not using the provided Jupyter notebook, make sure to also set the random seed as below.**\n",
    "\n",
    "**Important: Do not change any codes we give you below, except for those waiting for you to complete. This is to ensure your code has reproducible results and is important for grading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f788ad34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f788ad34",
    "outputId": "e6f1442e-c54d-4eaf-b2e3-90ca864d2d3f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "seed = 1234\n",
    "np.random.seed(seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35375",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1 (15 points): Draw a Decision Tree\n",
    "\n",
    "In the image below, you are given some data for a binary classification problem with features $X_1$ and $X_2$ and labels $y = 0$ or $y = 1$. In addition, the dashed black lines depict the splits of a particular decision tree trained on this data.\n",
    "\n",
    "- Draw the decision tree corresponding to the splits in the image. For every leaf node, you should also include $p(y = 1 | \\text{path})$. \n",
    "- Write your answer on paper and include a picture of your answer in this notebook. In order to include an image in Jupyter notebook, save the image in the same directory as the .ipynb file and then write `![caption](image.png)`. Alternatively, you may go to Edit --> Insert Image at the top menu to insert an image into a Markdown cell. Double check that your image is visible in your PDF submission.\n",
    "\n",
    "\n",
    "<img src=\"./problem1_tree.png\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d739d",
   "metadata": {},
   "source": [
    "Below is my image for problem 1\n",
    "\n",
    "<img src=\"./cs178HW4prob1.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54a56",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2 (15 points): Computing the Gini Index\n",
    "\n",
    "In the table below, you are given some data for a classification problem with 3 features and 3 classes. The feature $X_1$ is a binary feature, and the features $X_2$ and $X_3$ are real-valued features. The three classes are labeled $y = 0, 1, 2$. \n",
    "\n",
    "In this problem, you will use the Gini index to decide which feature to split on. In other words, you will be creating the root of a decision tree based on this data. You should do this problem by hand, and include an image (or LaTeX) with your solution in your notebook. See Problem 1 for details on how to include an image in your notebook.\n",
    "\n",
    "- What is the Gini index of the data before doing any splitting?\n",
    "- For each feature, compute the Gini index of splitting on that feature. Use a threshold of $t = 0.4$ for $X_2$ and $t = 2.0$ for $X_3$. (Why don't we need to specify a threshold for $X_1$? You don't have to answer this, but you should think about it.) \n",
    "- Based on your answer to the previous question, which feature should we split on for the root of our decision tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f19dc",
   "metadata": {},
   "source": [
    "| X1 | X2   | X3   | y |\n",
    "|----|------|------|---|\n",
    "| 0  | 0.1  | 3.4  | 0 |\n",
    "| 1  | 0.3  | 3.0  | 0 |\n",
    "| 0  | -0.2 | 2.9  | 0 |\n",
    "| 0  | 1.3  | 0.1  | 1 |\n",
    "| 1  | 2.2  | -0.5 | 1 |\n",
    "| 0  | 4.0  | 0.3  | 1 |\n",
    "| 1  | 0.5  | 1.2  | 2 |\n",
    "| 1  | 3.0  | 0.75 | 2 |\n",
    "| 1  | 2.2  | 0.1  | 2 |\n",
    "| 0  | 0.25 | 0.1  | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe01153a",
   "metadata": {},
   "source": [
    "<font color = blue> I will include my calculations written by hand as well as a writeup using LATEX. I ultimately determined that we should use feature X1 for our root node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d475c13",
   "metadata": {},
   "source": [
    "<img src=\"./gini-1.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2b0f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef3196",
   "metadata": {},
   "source": [
    "You will now implement an algorithm for learning a decision tree from data. You are given some starter code below that you will need to complete. To keep things simple, your implementation will only work for binary classification.\n",
    "\n",
    "The class `Node` represents a single node in a decision tree. This class is already completed for you, and contains several useful attributes. In addition, the class `DecisionTree` is partially implemented for you. Before attempting this problem, it is important that you read and understand both of these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\" A class representing a node in a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        self.depth = depth         # What level of the tree this node is at; depth=0 is the root node\n",
    "        \n",
    "        self.split_feature = None  # The index of the feature that this node splits, if any <-- None = root node\n",
    "        self.threshold = None      # The threshold used to split the feature\n",
    "        \n",
    "        self.left_child = None     # A node object (or None) representing the left-hand child of this node \n",
    "        self.right_child = None    # A node object (or None) representing the right-hand child of this node\n",
    "        \n",
    "        self.probs = None          # A numpy array of length 2 representing [p(y=0), p(y=1)] at this node\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # Gives a nice looking representation if you call print on a node \n",
    "        \n",
    "        return f'DT Node: \\n -| Depth: {self.depth}' \\\n",
    "                        f'\\n -| Split feature: {self.split_feature}' \\\n",
    "                        f'\\n -| Threshold: {self.threshold}' \\\n",
    "                        f'\\n -| Probs: {self.probs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25dfc31",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3237539289.py, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 90\u001b[1;36m\u001b[0m\n\u001b[1;33m    X_L, y_L = # TODO\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\" A class representing a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=3):\n",
    "        self.root = None            # A Node object which is the root of our tree\n",
    "        self.max_depth = max_depth  # An integer representing the maximum allowed depth of the tree\n",
    "        \n",
    "    def class_prob_vector(self, y):\n",
    "        \"\"\" Given an array of labels y, compute p(y=0) and p(y=1).\n",
    "        \n",
    "        returns: a numpy array containing [p(y=0), p(y=1)].\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "        # binary classification, so your array should ever only have 2 values (shown above)\n",
    "        # to calculate the probability vector, determine the ratio of training points there \n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        numLabels = len(y)\n",
    "        pY_0 = y.count(0) / len(y)\n",
    "        pY_1 = y.count(1)/ len(y)\n",
    "        \n",
    "        return [pY_0, pY_1]\n",
    "        \n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        \n",
    "    def leaf_condition(self, node):\n",
    "        \"\"\" Given a Node object, returns True if this is a leaf node and False otherwise.\n",
    "        \n",
    "        A Node is considered a leaf node if all labels at the node belong to the same class,\n",
    "            or if the node is at the maximum allowed depth of the tree.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        pLen = len(node.probs) \n",
    "        return node.depth == max_depth or node.probs[0] == pLen or node.probs[1] == pLen\n",
    "        \n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        \n",
    "    def gini_score(self, X, y, i, threshold):\n",
    "        \"\"\" Given features X and labels y, computes the Gini index of splitting\n",
    "            the i-th feature at the given threshold.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        # gini_t = P_l * G_l + P_R + G_R\n",
    "        yLen = len(y)\n",
    "        leftThresh = [i for i in X if i > threshold] \n",
    "        rightThresh = [i for i in X if i < threshold]\n",
    "        \n",
    "        p_l = len(leftThresh) / yLen # probability a datapoint is left of threshold\n",
    "        p_r = len(rightThresh) / yLen\n",
    "        \n",
    "        G_l = 1 - ((leftThresh.count(0) / yLen) ** 2) - ((leftThresh.count(1) / yLen) ** 2)\n",
    "        G_r = 1 - ((rightThresh.count(0) / yLen) ** 2) - ((rightThresh.count(1) / yLen) ** 2)\n",
    "        \n",
    "        return (p_l * g_l) + (p_r * g_r)\n",
    "        \n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        \n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\" Given features X and labels y, finds the best split based on the Gini index.\n",
    "        \n",
    "        returns: an index corresponding to which feature we are splitting,\n",
    "            as well as the threshold we are splitting the feature at.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "        # Call gini_score\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fits the decision tree given features X and labels y.\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), 'X must be a numpy array'\n",
    "        assert isinstance(y, np.ndarray), 'y must be a numpy array'\n",
    "        \n",
    "        self.build_tree(X, y, 0)\n",
    "    \n",
    "    def build_tree(self, X, y, depth):\n",
    "        \"\"\" Recursively builds the decision tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a new node\n",
    "        node = Node(depth)\n",
    "        if depth == 0:\n",
    "            self.root = node\n",
    "            \n",
    "        # Get the class probabilities for this node\n",
    "        node.probs = self.class_prob_vector(y)\n",
    "                \n",
    "        # Check if this new node is a leaf node; otherwise, split it\n",
    "        if self.leaf_condition(node):\n",
    "            return node            \n",
    "        else:            \n",
    "            # Find which feature to split on and the splitting threshold\n",
    "            split_idx, split_threshold = self.find_best_split(X, y)\n",
    "            \n",
    "            # Create left/right splits of data based on split_idx, split_threshold\n",
    "            X_L, y_L = # TODO\n",
    "            X_R, y_R = # TODO\n",
    "            \n",
    "            # Recursively create the left/right nodes\n",
    "            node_L = # TODO\n",
    "            node_R = # TODO\n",
    "            \n",
    "            # Fill in node information\n",
    "            node.split_feature = # TODO\n",
    "            node.threshold = # TODO\n",
    "            node.left_child = # TODO\n",
    "            node.right_child = # TODO\n",
    "            \n",
    "            return node\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" After fitting the decision tree, this function can be called to make predictions\n",
    "            for every data point in the feature array X. \n",
    "        \"\"\"\n",
    "        y_hat = []\n",
    "        \n",
    "        for x in X:\n",
    "            # Make a prediction for every row in X\n",
    "            y_hat.append(self._predict(x))\n",
    "        \n",
    "        y_hat = np.array(y_hat)\n",
    "        return y_hat\n",
    "                        \n",
    "    def _predict(self, x):\n",
    "        \"\"\" Makes predictions on individual datapoints x.\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        \n",
    "        while True:\n",
    "            if self.leaf_condition(current_node):\n",
    "                # If we're at a leaf node, make a prediction based on the probabilities\n",
    "                probs = current_node.probs\n",
    "                y_hat = np.argmax(probs)\n",
    "                return y_hat\n",
    "            else:\n",
    "                # Otherwise, traverse the tree based on the splits\n",
    "                go_left = x[current_node.split_feature] <= current_node.threshold\n",
    "                if go_left:\n",
    "                    current_node = current_node.left_child\n",
    "                else:\n",
    "                    current_node = current_node.right_child\n",
    "                \n",
    "    \n",
    "    def __repr__(self):\n",
    "        # Pretty printing if we call print on our DecisionTree\n",
    "        return f'Decision Tree \\n -| Max Depth: {self.max_depth}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbe84a",
   "metadata": {},
   "source": [
    "### Problem 3.1 (5 points):\n",
    "\n",
    "- Complete the function `class_prob_vector`. This function takes in array of labels `y` and returns a numpy array containing $p(y=0)$ and $p(y=1)$.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all tests should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b83f08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use this code block to test your implementation in Problem 3.1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Don't change anything here -- just run it\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTree\u001b[49m(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m expected \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.6\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecisionTree' is not defined"
     ]
    }
   ],
   "source": [
    "# Use this code block to test your implementation in Problem 3.1\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "expected = np.array([0.4, 0.6])\n",
    "out = dt.class_prob_vector(y)\n",
    "\n",
    "print(f'Test 1 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "y = np.array([1, 1, 1])\n",
    "expected = np.array([0., 1.])\n",
    "out = dt.class_prob_vector(y)\n",
    "\n",
    "print(f'Test 2 passed: {np.array_equal(expected, out)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f315ae",
   "metadata": {},
   "source": [
    "### Problem 3.2 (10 points):\n",
    "\n",
    "In our decision tree implementation, we will consider a node to be a leaf node if either (a) all labels at the node belong to the same class, or (b) the node is at depth `max_depth`, where `max_depth` is an attribute of our `DecisionTree` that we can specify.\n",
    "\n",
    "- Complete the function `leaf_condition`. This function should take in a `Node` object and return True if this node is a leaf node (according to the above criteria) and False otherwise. You should be able to determine this based on the attributes already defined in the `Node` class.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all tests should pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.2\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "node = Node(depth=2)\n",
    "node.probs = np.array([0.5, 0.5])\n",
    "expected = False\n",
    "out = dt.leaf_condition(node)\n",
    "print(f'Test 1 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "node = Node(depth=3)\n",
    "node.probs = np.array([0.5, 0.5])\n",
    "expected = True\n",
    "out = dt.leaf_condition(node)\n",
    "print(f'Test 2 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "node = Node(depth=1)\n",
    "node.probs = np.array([1., 0.])\n",
    "expected = True\n",
    "out = dt.leaf_condition(node)\n",
    "print(f'Test 3 passed: {np.array_equal(expected, out)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83828938",
   "metadata": {},
   "source": [
    "### Problem 3.3 (10 points):\n",
    "\n",
    "Your next task is to implement a function that computes the Gini index.\n",
    "\n",
    "- Complete the function `gini_score`. This function takes in features `X` and labels `y`, as well as a feature index `i` and a scalar `threshold`. Given these inputs, the function `gini_score` should return the Gini index (i.e. a single number) obtained by splitting the `i`th feature in `X` at the specified threshold.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all tests should pass. Note that the numerical tests here are the same as the examples in Lectures 18 and 19, which may be helpful in debugging your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d10700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.3\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "# Note: these are the same values as the example in Lectures 18 and 19\n",
    "\n",
    "X = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "expected = 5/11\n",
    "out = dt.gini_score(X, y, 0, 1.5)\n",
    "print(f'Test 1 passed: {np.isclose(expected, out)}')\n",
    "\n",
    "expected = 2/5\n",
    "out = dt.gini_score(X, y, 0, 2.5)\n",
    "print(f'Test 2 passed: {np.isclose(expected, out)}')\n",
    "\n",
    "expected = 5/11\n",
    "out = dt.gini_score(X, y, 1, 2)\n",
    "print(f'Test 3 passed: {np.isclose(expected, out)}')\n",
    "\n",
    "expected = 4/9\n",
    "out = dt.gini_score(X, y, 1, 6.5)\n",
    "print(f'Test 4 passed: {np.isclose(expected, out)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7764c25",
   "metadata": {},
   "source": [
    "### Problem 3.4 (15 points): \n",
    "\n",
    "Now, you will use your function `gini_score` to compute the best splits. To do this, you will need to complete the function `find_best_split`. This function takes in features `X` and labels `y`, and returns the feature index and threshold corresponding to the best split as determined by `gini_score`. That is, for every feature and every threshold, you should compute the Gini index of splitting that feature at that threshold, and you should return the index of the feature and the threshold that results in the lowest Gini index.\n",
    "\n",
    "To determine the thresholds, we will use the **midpoint** strategy. That is, given an array of feature values, we will consider all thresholds given by the midpoints between consecutive feature values. Here's an example. Suppose we are given a feature matrix $X$ with four datapoints and two features, given by\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 1.5 & 2.5 \\\\ 0.75 & -1.0 \\\\ 3.0 & 0.5 \\end{bmatrix}.$$\n",
    "\n",
    "The thresholds to consider when splitting the second feature would then be $[-0.25, 1.25, 2.25]$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Complete the function `find_best_split` as detailed above.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all tests should pass. Note that the numerical tests here are the same as the examples in Lectures 18 and 19, which may be helpful in debugging your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.4\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "# Note: these are the same values as the example in Lectures 18 and 19\n",
    "\n",
    "X = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "expected = (0, 8.5)\n",
    "out = dt.find_best_split(X, y)\n",
    "print(f'Test 1 passed: {expected == out}')\n",
    "\n",
    "X_L, y_L = X[X[:, 0] <= 8.5], y[X[:, 0] <= 8.5]\n",
    "expected = (1, 6)\n",
    "out = dt.find_best_split(X_L, y_L)\n",
    "print(f'Test 2 passed: {expected == out}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e081cd",
   "metadata": {},
   "source": [
    "### Problem 3.5 (10 points):\n",
    "\n",
    "For the last step in implementing `DecisionTree`, you will need to complete the function `build_tree`. This function uses the functions you implemented in Problems 3.1-3.4 in order to recursively build your decision tree.\n",
    "\n",
    "- Complete the function `build_tree`. Fill in the lines of code marked `#TODO`.\n",
    "- Run the code block given below to test your implementation. If your code is correct, all tests should pass. Note that the *first test* here is the same as the examples in Lectures 18 and 19, which may be helpful in debugging your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ecd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code block to test your implementation in Problem 3.5\n",
    "# Don't change anything here -- just run it\n",
    "\n",
    "\n",
    "# Note: these are the same values as the example in Lectures 18 and 19\n",
    "X = np.array([[1,1], [2,8], [4,9], [6,7], [7,4], [8,11], [3,3], [5,5], [9,5], [10,8], [11, 6], [12,10]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "expected = np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "dt = DecisionTree(max_depth=3)\n",
    "dt.fit(X, y)\n",
    "out = dt.predict(X)\n",
    "print(f'Test 1 passed: {np.array_equal(expected, out)}')\n",
    "\n",
    "X, y = make_classification(n_samples = 250, n_features=5, n_informative=3, random_state=seed)\n",
    "sklearn_dt = DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=seed)\n",
    "sklearn_dt.fit(X, y)\n",
    "expected = sklearn_dt.predict(X)\n",
    "dt = DecisionTree(max_depth=3)\n",
    "dt.fit(X, y)\n",
    "out = dt.predict(X)\n",
    "print(f'Test 2 passed: {np.array_equal(expected, out)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978e19e",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Experimenting with Decision Trees\n",
    "\n",
    "In the final problem of this assignment, you will experiment with the scikit-learn implementation of decision trees on the Wine dataset. This dataset consists of 178 datapoints with 13 real-valued features and 3 possible labels. The features correspond to a chemical analysis of various wines, and the labels correspond to which type of wine we are considering. See the documentation [here](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset) for some more information on this dataset.\n",
    "\n",
    "Before attempting this problem, you should read and understand the documentation for the `DecisionTreeClassifier`, available [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "**Important: for every decision tree classifier you train in this problem, make sure to set `random_state=seed` for reproducibility.**\n",
    "\n",
    "We will first load in this dataset and create a train/test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482aa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_wine(return_X_y=True)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a18ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8dc7ac",
   "metadata": {},
   "source": [
    "### Problem 4.1 (5 points):\n",
    "\n",
    "First, you will see how to fit and visualize a decision tree classifier in sklearn.\n",
    "\n",
    "- Using the class `DecisionTreeClassifier` from scikit-learn, train a decision tree on the training data. Use `max_depth=2`, and leave all other settings as their defaults. Note that, by default, `DecisionTreeClassifier` uses the Gini index to split nodes.\n",
    "- What is the training set error and testing set error of your classifier?\n",
    "- Use the function `plot_tree` to visualize your decision tree. This is already imported for you at the top of this notebook. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) for the corresponding documentation page.\n",
    "- For the first split of the data, what feature and what threshold is being used in your classifier? You don't need to find the name of this feature, just its index, i.e. which column of `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99d666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "001fac32",
   "metadata": {},
   "source": [
    "### Problem 4.2 (5 points):\n",
    "\n",
    "You will now vary the maximum depth allowed in your decision tree and see what effect this has on the error rate.\n",
    "\n",
    "- Train a decision tree for every value of `max_depth` in `[1, 2, ..., 15]`. Use the default settings (other than `max_depth`). \n",
    "- Plot the resulting training and testing set accuracies as a function of depth. Be sure to include an x-label, a y-label, and a legend in your plot.\n",
    "- Describe what you see happen as you increase the depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46fe3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a15368",
   "metadata": {},
   "source": [
    "### Problem 4.3 (5 points):\n",
    "\n",
    "Lastly, you will vary the minimum number of datapoints allowed in a leaf node in your decision tree and see what effect this has on the error rate.\n",
    "\n",
    "- Train a decision tree for every value of `min_samples_leaf` in `[1, 2, ..., 15]`. Use `max_depth=3` and use the default settings for all other parameters.\n",
    "- Plot the resulting training and testing set accuracies as a function of the minimum leaf samples. Be sure to include an x-label, a y-label, and a legend in your plot.\n",
    "- You should see that the training error increases as we increase the minimum number of leaf samples. Give an explanation for why this might happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9f5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a052765",
   "metadata": {
    "id": "4a052765"
   },
   "source": [
    "---\n",
    "### Statement of Collaboration (5 points)\n",
    "\n",
    "It is **mandatory** to include a Statement of Collaboration in each submission, with respect to the guidelines below. Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed. If you did not collaborate with anyone, you should write something like \"I completed this assignment without any collaboration.\"\n",
    "\n",
    "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
    "programming assignments, in particular, I encourage the students to organize (perhaps using EdD) to\n",
    "discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start\n",
    "working on it. However, you should not discuss the specific solutions, and, as a guiding principle, you are not\n",
    "allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard,\n",
    "written notes, referring to EdD, etc.). Especially after you have started working on the assignment, try\n",
    "to restrict the discussion to EdD as much as possible, so that there is no doubt as to the extent of your\n",
    "collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e7824",
   "metadata": {},
   "source": [
    "<font color = blue> I did not work with anyone else for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3093043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
